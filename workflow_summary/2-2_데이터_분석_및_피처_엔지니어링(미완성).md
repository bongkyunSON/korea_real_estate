# [Phase 2] 데이터 파이프라인 구축

## 2-2. 데이터 분석 및 피처 엔지니어링: '원석'에서 '보석'을 꿰는 과정

### 1. 개요

Phase 2-1에서 우리는 데이터라는 '원석'을 자동으로 채굴하는 '광산(Airflow 파이프라인)'을 성공적으로 건설했습니다. 하지만 원석은 그 자체로는 가치를 제대로 발휘하지 못합니다. AI라는 정교한 기계를 작동시키기 위해서는, 이 원석을 정밀하게 깎고 다듬어 반짝이는 '보석(Feature)'으로 만드는 과정이 반드시 필요합니다.

이번 단계의 목표는 데이터 세공의 장인이 되어, Jupyter Notebook이라는 작업대 위에서 원시 데이터를 탐험하고, 숨겨진 패턴을 찾아내며, 최종적으로 AI 모델의 성능을 극대화할 수 있는 강력한 피처들을 설계하고 이를 자동화하는 스크립트(`build_features.py`)를 완성하는 것이었습니다.

### 2. 접근 전략: '데이터 탐험가의 항해일지' 비유

우리는 미지의 데이터 대륙을 탐험하는 탐험가처럼 체계적인 단계를 밟았습니다.

- **Phase 2-2A: 탐험 준비 (Jupyter Notebook 환경 구축)**
  - 어떤 제약도 없이 자유롭게 데이터를 맛보고, 실험하고, 시각화할 수 있는 탐험 기지, `Jupyter Notebook`을 설치하고 데이터베이스와 연결했습니다.

- **Phase 2-2B: 신대륙 탐험 및 지도 제작 (EDA 및 피처 엔지니어링)**
  - 매매 데이터와 전월세 데이터라는 두 개의 신대륙을 탐험하며 지도를 제작(EDA)했습니다. '평당 가격'이라는 주요 산맥을 발견하고, '자치구별/동별 시장 특성'이라는 주요 강줄기를 그려 넣었습니다.

- **Phase 2-2C: 숨겨진 보물 발굴 (핵심 인사이트 '갭투자' 분석)**
  - 탐험의 하이라이트는 주니어님의 번뜩이는 아이디어에서 시작된 '갭투자'라는 숨겨진 보물섬의 발견이었습니다. 우리는 두 대륙의 지도를 겹쳐보고, 특정 조건(매매일이 전세계약 기간에 포함)에 맞는 지점을 찾아내 보물의 위치를 정확히 특정하는 데 성공했습니다.

- **Phase 2-2D: 최적의 항해 경로 확정 (분석 로직 스크립트화)**
  - 탐험을 통해 얻은 모든 지식과 보물 지도를 종합하여, `build_features.py`라는 '최적의 항해 경로'를 완성했습니다. 이제 누구든 이 스크립트만 실행하면, 우리가 겪었던 모든 탐험 과정을 재현하고 똑같은 보물을 찾아낼 수 있게 되었습니다.

### 3. 분석 과정에서 마주한 도전과 해결

탐험 과정은 언제나처럼 순탄치만은 않았습니다. 우리는 여러 난관을 마주쳤지만, 논리적인 추론을 통해 모두 해결했습니다.

#### 🧐 도전 1: 숫자로 보이지만 숫자가 아니었던 데이터

- **현상**: `df.describe(include='number')` 코드가 `ValueError: No objects to concatenate` 에러를 뿜어냈습니다. 모든 데이터가 `object`(문자열) 타입이었고, 특히 `dealAmount` 컬럼에는 쉼표(`,`)가 섞여 있어 숫자 변환이 실패했던 것입니다.
- **해결**: `.str.replace(',', '')`로 쉼표를 제거하고, `pd.to_numeric` 함수의 `errors='coerce'` 옵션을 사용하여 변환할 수 없는 값은 NaN으로 만드는 안전한 방식으로 모든 숫자 컬럼을 성공적으로 변환했습니다.

#### 🧐 도전 2: 이름만 보고 같은 사람인 줄 알았던 데이터

- **현상**: 매매 데이터를 처리했던 경험을 바탕으로 전월세 데이터에서 불필요한 컬럼을 제거하려 하자 `KeyError`가 발생했습니다. 두 데이터셋이 비슷해 보였지만, 실제 컬럼 이름은 미묘하게 달랐습니다.
- **해결**: '추측' 대신 '확인'을 선택했습니다. `df_jeonse.isnull().sum()`으로 실제 컬럼명과 결측치 현황을 직접 눈으로 확인한 뒤, 정확한 컬럼명을 사용하여 문제를 해결했습니다.

#### 🧐 도전 3: 데이터에 숨겨진 시대적 배경의 발견

- **현상**: '갭투자' 분석 로직을 처음 완성했을 때, 우리는 모든 기간의 데이터를 대상으로 분석했습니다.
- **발견 (주니어님의 핵심 인사이트)**: 주니어님께서 "2021년 6월 주택임대차 신고제 법제화"라는 결정적인 도메인 지식을 제공해주셨습니다. 이로 인해 2022년 이전의 `contractTerm` 데이터는 신뢰도가 낮다는 것을 깨달았습니다.
- **해결**: 분석의 정확도를 높이기 위해, `deal_datetime >= '2022-01-01'` 조건을 추가하여 신뢰할 수 있는 기간의 데이터만을 대상으로 분석을 진행했습니다. 이는 기술적 해결을 넘어, **데이터의 생성 배경을 이해하는 것이 얼마나 중요한지 보여준 최고의 사례**였습니다.

### 4. 최종 결과물: `build_features.py`

길고 긴 탐험과 분석 끝에 완성된, 우리의 모든 지식과 로직이 집약된 최종 스크립트입니다.

```python
import os
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
from dotenv import load_dotenv
import warnings

# 경고 메시지 무시
warnings.filterwarnings('ignore', category=UserWarning, module='pandas')

def get_db_engine():
    """데이터베이스 연결 엔진을 생성하고 반환합니다."""
    dotenv_path = os.path.join(os.path.dirname(os.getcwd()), '.env')
    if not os.path.exists(dotenv_path):
         dotenv_path = os.path.join(os.getcwd(), '.env')
            
    load_dotenv(dotenv_path=dotenv_path)
    
    database_url = os.getenv("DATABASE_URL_HOST")
    if not database_url:
        raise ValueError("DATABASE_URL_HOST 환경 변수가 설정되지 않았습니다.")
    
    return create_engine(database_url)

def process_trade_data(engine, schema):
    """매매 데이터를 처리하고 피처를 생성합니다."""
    print("--- [1/4] 매매 데이터 처리 시작 ---")
    
    print(">> raw_apt_trade 테이블 로딩 중...")
    df = pd.read_sql(f'SELECT * FROM {schema}."raw_apt_trade"', engine)
    print(f">> 매매 데이터 {len(df)}건 로딩 완료.")

    # 데이터 정제 및 타입 변환
    df['dealAmount'] = pd.to_numeric(df['dealAmount'].str.replace(',', '').str.strip(), errors='coerce')
    numeric_cols = ['excluUseAr', 'dealYear', 'dealMonth', 'dealDay', 'buildYear', 'floor']
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # 결측치 및 이상치 처리
    df.dropna(subset=['dealAmount', 'excluUseAr'], inplace=True)
    df = df[df['floor'] > 0].copy()

    # 피처 엔지니어링
    df['deal_datetime'] = pd.to_datetime(df['dealYear'].astype(str) + '-' + 
                                       df['dealMonth'].astype(str) + '-' + 
                                       df['dealDay'].astype(str), errors='coerce')
    df['price_per_pyeong'] = df['dealAmount'] / (df['excluUseAr'] / 3.3058)

    sgg_map = df.groupby('sggCd')['sggNm'].first().to_dict()
    df['sggNm'] = df['sggCd'].map(sgg_map)

    dong_avg_price = df.groupby(['sggNm', 'umdNm'])['price_per_pyeong'].transform('mean')
    df['dong_avg_price'] = dong_avg_price
    
    # 불필요한 컬럼 제거
    final_cols = [
        'sggCd', 'umdCd', 'jibun', 'aptNm', 'excluUseAr', 'floor', 'buildYear', 
        'dealAmount', 'deal_datetime', 'sggNm', 'umdNm', 
        'price_per_pyeong', 'dong_avg_price'
    ]
    df_final = df[final_cols].copy()
    
    print("--- 매매 데이터 처리 완료 ---")
    return df_final

def process_rent_data(engine, schema):
    """전월세 데이터를 처리하고 피처를 생성합니다."""
    print("--- [2/4] 전월세 데이터 처리 시작 ---")
    
    print(">> raw_apt_jeonse 테이블 로딩 중...")
    df = pd.read_sql(f'SELECT * FROM {schema}."raw_apt_jeonse"', engine)
    print(f">> 전월세 데이터 {len(df)}건 로딩 완료.")

    # 데이터 정제 및 타입 변환
    numeric_cols = [
        'deposit', 'monthlyRent', 'excluUseAr', 'buildYear', 'dealYear', 
        'dealMonth', 'dealDay', 'floor', 'preDeposit', 'preMonthlyRent'
    ]
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    df.dropna(subset=['deposit', 'monthlyRent', 'excluUseAr'], inplace=True)

    # 피처 엔지니어링
    df['rent_type'] = np.where(df['monthlyRent'] == 0, '전세', '월세')
    df['deal_datetime'] = pd.to_datetime(df['dealYear'].astype(str) + '-' + 
                                       df['dealMonth'].astype(str) + '-' + 
                                       df['dealDay'].astype(str), errors='coerce')

    sgg_map = df.groupby('sggCd')['sggNm'].first().to_dict()
    df['sggNm'] = df['sggCd'].map(sgg_map)
    
    # 계약 기간 파싱
    term = df['contractTerm'].str.split('~', expand=True)
    df['contract_start_date'] = pd.to_datetime('20' + term[0].str.replace('.', '-'), errors='coerce')
    df['contract_end_date'] = pd.to_datetime('20' + term[1].str.replace('.', '-'), errors='coerce')

    # 시장 특성 피처 생성
    df_jeonse = df[df['rent_type'] == '전세'].copy()
    df_wolse = df[df['rent_type'] == '월세'].copy()

    df_jeonse['price_per_pyeong'] = df_jeonse['deposit'] / (df_jeonse['excluUseAr'] / 3.3058)
    df_wolse['deposit_per_pyeong'] = df_wolse['deposit'] / (df_wolse['excluUseAr'] / 3.3058)

    # 구별/동별 평균값 계산
    for group_col in ['sggNm', 'umdNm']:
        # 전세
        jeonse_avg = df_jeonse.groupby(group_col)['price_per_pyeong'].transform('mean')
        df[f'{group_col}_avg_pp_jeonse'] = jeonse_avg
        # 월세
        wolse_deposit_avg = df_wolse.groupby(group_col)['deposit_per_pyeong'].transform('mean')
        df[f'{group_col}_avg_pp_wolse_deposit'] = wolse_deposit_avg
        wolse_rent_avg = df_wolse.groupby(group_col)['monthlyRent'].transform('mean')
        df[f'{group_col}_avg_monthly_rent'] = wolse_rent_avg

    final_cols = [
        'sggCd', 'umdCd', 'jibun', 'aptNm', 'excluUseAr', 'floor', 'buildYear', 'deposit', 'monthlyRent',
        'deal_datetime', 'sggNm', 'umdNm', 'rent_type', 'contract_start_date', 'contract_end_date',
        'sggNm_avg_pp_jeonse', 'umdNm_avg_pp_jeonse', 'sggNm_avg_pp_wolse_deposit',
        'umdNm_avg_pp_wolse_deposit', 'sggNm_avg_monthly_rent', 'umdNm_avg_monthly_rent'
    ]
    df_final = df[final_cols].copy()
    
    print("--- 전월세 데이터 처리 완료 ---")
    return df_final

def analyze_gap_investment(df_trade, df_rent):
    """갭투자 데이터를 분석합니다."""
    print("--- [3/4] 갭투자 분석 시작 ---")

    sales_df = df_trade[df_trade['deal_datetime'].dt.year >= 2022].copy()
    sales_df['deal_year'] = sales_df['deal_datetime'].dt.year

    jeonse_df = df_rent[
        (df_rent['rent_type'] == '전세') &
        (df_rent['contract_start_date'].notna()) &
        (df_rent['contract_end_date'].dt.year >= 2022)
    ].copy()

    apartment_key = ['sggNm', 'umdNm', 'jibun', 'aptNm', 'excluUseAr', 'floor']
    sales_df['sale_id'] = range(len(sales_df))

    total_sales = sales_df.groupby(['deal_year', 'sggNm', 'umdNm']).size().rename('총 매매 건수')

    merged_df = pd.merge(sales_df, jeonse_df[apartment_key + ['contract_start_date', 'contract_end_date']], on=apartment_key, how='inner')
    
    gap_mask = (merged_df['deal_datetime'] >= merged_df['contract_start_date']) & \
               (merged_df['deal_datetime'] <= merged_df['contract_end_date'])

    unique_gap_deals = merged_df[gap_mask].drop_duplicates(subset=['sale_id'])
    gap_counts = unique_gap_deals.groupby(['deal_year', 'sggNm', 'umdNm']).size().rename('갭투자 건수')

    summary_df = pd.concat([total_sales, gap_counts], axis=1).fillna(0).astype(int)
    summary_df['갭투자 비율(%)'] = ((summary_df['갭투자 건수'] / summary_df['총 매매 건수']) * 100).round(2)
    
    print(f">> 갭투자 분석 완료. 총 {len(summary_df)}개 동별 데이터 생성.")
    print("--- 갭투자 분석 완료 ---")
    return summary_df.reset_index()

def save_to_db(df, table_name, engine, schema):
    """데이터프레임을 데이터베이스 테이블에 저장합니다."""
    print(f">> '{table_name}' 테이블 저장 중... ({len(df)}건)")
    df.to_sql(table_name, engine, schema=schema, if_exists='replace', index=False)
    print(f">> '{table_name}' 테이블 저장 완료.")

def main():
    """메인 실행 함수"""
    engine = get_db_engine()
    schema = os.getenv("DB_SCHEMA", "public")
    
    # 1. 매매 데이터 처리 및 저장
    feature_trade_df = process_trade_data(engine, schema)
    
    # 2. 전월세 데이터 처리 및 저장
    feature_rent_df = process_rent_data(engine, schema)
    
    # 3. 갭투자 분석
    analytics_gap_df = analyze_gap_investment(feature_trade_df, feature_rent_df)
    
    # 4. 최종 테이블 저장
    print("\n--- [4/4] 최종 데이터베이스 저장 시작 ---")
    save_to_db(feature_trade_df, "feature_apt_trade", engine, schema)
    save_to_db(feature_rent_df, "feature_apt_rent", engine, schema)
    save_to_db(analytics_gap_df, "analytics_gap_investment", engine, schema)
    print("--- 모든 작업이 성공적으로 완료되었습니다. ---")

if __name__ == "__main__":
    main()
```

### 5. 결론 및 다음 단계

우리는 마침내 원시 데이터를 AI가 학습할 수 있는 최종 피처 데이터로 변환하는 모든 로직을 `build_features.py`라는 하나의 자동화 스크립트에 담아내는 데 성공했습니다. 이것으로 데이터 파이프라인의 '두뇌' 설계가 완료되었습니다.

이제 Phase 2의 마지막 퍼즐 조각을 맞출 시간입니다.

- **Phase 2-3: 데이터 파이프라인 통합**
  - Phase 2-1에서 만든 '원석 채굴기(Airflow DAG)'가 원석을 다 캐면, 우리가 Phase 2-2에서 만든 '보석 세공기(`build_features.py`)'가 자동으로 이어서 작동하도록 두 시스템을 연결하는 작업을 시작합니다.
  - 이 통합이 끝나면, 우리는 비로소 **'원석 채굴부터 보석 세공까지' 모든 과정이 자동으로 이루어지는 완벽한 데이터 파이프라인**을 갖게 될 것입니다.
