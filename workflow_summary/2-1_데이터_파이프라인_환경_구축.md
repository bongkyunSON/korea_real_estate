# Phase 2: 부동산 데이터 수집 파이프라인 구축

## 1. 개요

백엔드 서버가 AI 비서의 '뇌'라면, 데이터 파이프라인은 신선한 정보를 꾸준히 공급하는 '혈관'과 같습니다. 우리는 이 혈관 시스템을 구축하기 위해, 데이터 수집, 처리, 저장을 자동화하고 스케줄링하는 도구인 **Airflow**를 도입하기로 했습니다.

이번 단계의 목표는 국토교통부 API 등에서 부동산 데이터를 주기적으로 가져오는 작업을 실행할 Airflow 서버를 **Docker**를 이용해 안정적으로 구축하고, 실제 데이터 수집 로직을 담은 첫 번째 **DAG(Directed Acyclic Graph)**를 완성하는 것이었습니다.

## 2. 접근 전략: '도서관 건립' 비유

우리는 무작정 자동화부터 시작하는 대신, 체계적인 데이터 엔지니어링 접근법을 따랐습니다.

- **Phase 2-A: 도서관 개관 준비 (과거 데이터 대량 적재 - Backfill)**
  - 도서관을 처음 열 때, 세상에 있는 모든 책(과거 2년 치 데이터)을 한 번에 다 서가에 꽂는 대규모 작업을 진행했습니다. 이 작업은 일회성 파이썬 스크립트(`backfill_past_data.py`)로 성공적으로 수행했습니다.
- **Phase 2-B: 신간 입고 관리 (최신 데이터 증분 적재 - Incremental Load)**
  - 도서관이 문을 연 후, 매달 새로 나오는 신간(최신 데이터)만 확인해서 우리 도서관에 없는 책만 골라 서가에 추가하는 일상적인 업무입니다. 이 '일상적이고 반복적인 업무'를 **Airflow DAG**로 자동화하는 것이 최종 목표였습니다.

## 3. 버그 사냥일지: Airflow와의 기나긴 사투

`backfill` 스크립트의 성공 이후, 우리는 Airflow 환경 구축과 DAG 개발에 착수했습니다. 하지만 이 과정은 순탄치 않았고, 우리는 마치 탐정이 된 것처럼 단서를 하나씩 찾아가며 문제의 근원을 파헤쳐야 했습니다.

#### 🐞 버그 1: `AttributeError: 'str' object has no attribute 'strftime'`

- **현상**: DAG의 첫 번째 Task부터 실패. Airflow가 날짜 정보를 문자열로 넘겨주는데, 코드에서는 날짜 객체처럼 다루려고 해서 발생.
- **처방 1**: `pendulum.parse()`를 사용해 문자열을 날짜 객체로 변환하도록 코드를 수정.
- **미스터리**: 코드를 수정했음에도 불구하고, Airflow는 계속 예전 코드를 실행하며 똑같은 오류를 발생시킴.
- **진단**: Airflow 내부에 보이지 않는 **'캐시'**가 남아있거나, Docker 볼륨이 꼬여서 변경사항이 제대로 반영되지 않는 깊은 수준의 문제로 판단.
- **최종 처방**: `docker-compose down --volumes` 명령어로 컨테이너뿐만 아니라 데이터가 저장되는 **볼륨까지 완전히 삭제**하여 Airflow를 '공장 초기화' 상태로 되돌려 해결.

#### 🐞 버그 2: `ModuleNotFoundError: No module named 'utils'`

- **현상**: DAG 파일이 `utils` 폴더에 있는 `get_region_codes.py`를 찾지 못함.
- **수사 과정**:
  1. **가설 1**: "컨테이너 안에 `utils` 폴더가 없다?" -> `docker exec ls`로 확인. **폴더는 존재했음.**
  2. **가설 2**: "파이썬이 `utils` 폴더를 쳐다보지 않는다?" -> `docker-compose.yml`에 `PYTHONPATH=/opt/airflow` 환경 변수를 추가하여 파이썬의 '시야'를 넓혀줌.
  3. **가설 3**: "`utils` 폴더가 '패키지'로 인식되지 않는다?" -> 파이썬에게 이 폴더가 공식 패키지임을 알려주는 '목차' 역할의 **`utils/__init__.py`** 파일을 추가.
- **결론**: 위 2, 3번 처방을 통해 `utils` 모듈을 성공적으로 인식시킴.

#### 🐞 버그 3: 모든 문제의 근원, `AttributeError: '...Connection/Engine...' object has no attribute 'cursor'`

- **현상**: `ModuleNotFoundError`를 해결하자, `backfill` 스크립트에서 봤던 `cursor` 관련 에러가 재발.
- **수사 과정**:
  1. **시도 1**: `con=connection` vs `con=engine`. 어떤 연결 방식을 써도 실패.
  2. **시도 2 (잘못된 처방)**: `backfill` 때 성공했던 `SQLAlchemy==2.0.30` 버전을 `requirements.txt`에 명시. -> **Airflow가 `SQLAlchemy<2.0` 버전을 요구**하여 의존성 충돌 발생. **빌드 실패.**
  3. **시도 3 (결정적 단서)**: Airflow의 요구에 맞춰 `SQLAlchemy` 버전을 자유롭게 두는 대신, `pandas` 버전을 구형 `SQLAlchemy`와 호환되는 **`1.5.3`으로 다운그레이드**.
  4. **범인 검거**: `pandas==1.5.3`을 설치하려 하자, `ERROR: Could not build wheels for pandas` 에러 발생. `pandas 1.5.3`은 **파이썬 3.12를 지원하지 않기 때문**이었음.
- **최종 진단**: 모든 문제의 근원은 **"Airflow의 구형 SQLAlchemy"**와 **"우리의 최신 파이썬 3.12"** 라는 두 가지 상충하는 요구조건 사이에 끼어있던 **'Pandas 버전'** 이었음.

#### ✨ 최종 해결책: 환경 자체의 교체

- **처방**: `Dockerfile`의 기반 이미지를 `apache/airflow:2.9.2-python3.12`에서 **`apache/airflow:2.9.2-python3.11`**으로 변경.
- **결과**: **파이썬 3.11 환경** 위에서 **Airflow(SQLAlchemy 1.x 요구)**와 **Pandas 1.5.3**이 드디어 평화롭게 공존하게 되었고, 모든 문제가 해결됨.

## 4. 최종 결과물: `fetch_real_estate_data_dag.py`

길고 긴 디버깅 끝에 완성된, 안정적으로 동작하는 최종 DAG 코드입니다.

```python
from __future__ import annotations

import os
from datetime import datetime

import pendulum
from airflow.decorators import dag, task

@dag(
    dag_id="fetch_real_estate_data",
    # 매월 1일 새벽 3시에 실행되도록 설정합니다.
    schedule="0 3 1 * *",
    start_date=pendulum.datetime(2024, 1, 1, tz="Asia/Seoul"),
    catchup=False,
    tags=["real_estate", "api", "final"],
)
def fetch_real_estate_data_dag():
    """
    매월 초, 국토교통부 API를 통해 '지난달'의 서울시 전체 아파트 매매 실거래가
    데이터를 수집하여 DB에 증분 적재하는 DAG입니다.
  
    데이터 처리 방식:
    1. DB에서 해당 월의 기존 데이터를 읽어옵니다.
    2. API에서 해당 월의 신규 데이터를 모두 가져옵니다.
    3. 두 데이터를 비교하여 중복되지 않는 '순수 신규' 데이터만 DB에 추가합니다.
    """

    @task
    def get_target_month(data_interval_start: str) -> str:
        """
        Airflow가 전달한 날짜 '문자열'을 '날짜 객체'로 변환한 후,
        '지난달'을 YYYYMM 형식의 문자열로 변환합니다.
        """
        execution_date = pendulum.parse(data_interval_start)
      
        target_month_str = execution_date.strftime("%Y%m")
        print(f"이번 작업의 대상 월은 '{target_month_str}' 입니다.")
        return target_month_str

    @task
    def process_data_for_month(target_month: str):
        """
        특정 월의 데이터를 '비교 후 증분 적재' 방식으로 처리합니다.
        모든 외부 라이브러리 import와 객체 생성은 이 Task 내부에서 수행됩니다.
        """
        # --- 1. Task 내부에서 모든 의존성 import 및 초기화 ---
        import sys
        import pandas as pd
        from dotenv import load_dotenv
        from sqlalchemy import create_engine, text
        from PublicDataReader import TransactionPrice

        # 이 sys.path.append는 PYTHONPATH 환경 변수 설정으로 사실상 불필요해졌지만,
        # 만약을 대비한 2중 안전장치로 남겨둡니다.
        sys.path.append(os.path.dirname(os.path.dirname(__file__)))
        from utils.get_region_codes import get_seoul_sigungu_codes
      
        load_dotenv()
        DATABASE_URL = os.getenv("DATABASE_URL")
        PUBLIC_DATA_API_KEY = os.getenv("PUBLIC_DATA_HUB")
        DB_SCHEMA = os.getenv("DB_SCHEMA", "public")

        if not PUBLIC_DATA_API_KEY or not DATABASE_URL:
            raise ValueError("API 키 또는 데이터베이스 URL이 .env 파일에 설정되지 않았습니다.")
      
        engine = create_engine(DATABASE_URL, pool_pre_ping=True, pool_recycle=3600)
        api = TransactionPrice(PUBLIC_DATA_API_KEY)

        print(f"'{target_month}' 데이터 처리 시작 (비교 후 증분 적재 방식)")
        table_name = "raw_apt_trade"

        # --- 2. DB에서 기존 데이터 읽어오기 ---
        try:
            with engine.begin() as connection:
                query = text(f'SELECT * FROM {DB_SCHEMA}."{table_name}" WHERE "수집월" = :month')
                existing_df = pd.read_sql(query, connection, params={"month": target_month})
            print(f">> [DB 읽기] 성공. '{target_month}'월의 기존 데이터 {len(existing_df)}건을 찾았습니다.")
        except Exception as e:
            print(f">> [DB 읽기] 실패 또는 데이터 없음: {e}")
            existing_df = pd.DataFrame()

        # --- 3. API에서 신규 데이터 수집하기 ---
        print(f">> [API 수집] 시작. '{target_month}'월의 신규 데이터를 수집합니다.")
        seoul_codes = get_seoul_sigungu_codes()
        api_df_list = []
        for district, code in seoul_codes.items():
            try:
                original_df = api.get_data(
                    property_type="아파트",
                    trade_type="매매",
                    sigungu_code=code,
                    year_month=target_month,
                )
                if not original_df.empty:
                    api_df_list.append(pd.DataFrame(original_df))
            except Exception as e:
                print(f"   - {district} API 호출 중 오류 발생 (건너뜁니다): {e}")
      
        if not api_df_list:
            print(">> [API 수집] 신규 데이터가 없어 작업을 중단합니다.")
            return
      
        api_df = pd.concat(api_df_list, ignore_index=True)
        print(f">> [API 수집] 성공. 총 {len(api_df)}건의 데이터를 API로부터 가져왔습니다.")

        # --- 4. 데이터 비교 및 신규 데이터만 필터링 ---
        print(">> [데이터 비교] 시작. 기존 데이터와 신규 데이터를 비교합니다.")
        if not existing_df.empty:
            existing_df = existing_df.reindex(columns=api_df.columns).fillna(0)
          
        combined_df = pd.concat([api_df, existing_df], ignore_index=True)
        new_data_df = combined_df.drop_duplicates(keep=False)

        # --- 5. 신규 데이터가 있을 경우에만 DB에 저장 ---
        if new_data_df.empty:
            print(">> [최종 저장] 신규 데이터가 없습니다. 작업을 종료합니다.")
            return

        print(f">> [최종 저장] {len(new_data_df)}건의 새로운 데이터를 DB에 저장합니다.")
        try:
            with engine.begin() as connection:
                new_data_df.to_sql(
                    name=table_name,
                    con=connection,
                    schema=DB_SCHEMA,
                    if_exists="append",
                    index=False,
                )
            print(">> [최종 저장] 성공!")
        except Exception as e:
            print(f">> [최종 저장] 실패: {e}")
            raise e

    # --- Task 실행 순서 정의 ---
    target_month_value = get_target_month(data_interval_start="{{ data_interval_start }}")
    process_data_for_month(target_month=target_month_value)

# Airflow가 DAG 객체를 인식할 수 있도록 변수에 할당합니다.
fetch_real_estate_data = fetch_real_estate_data_dag()
```

## 5. 결론 및 다음 단계

길고 험난한 여정 끝에, 우리는 마침내 **매달 자동으로 부동산 데이터를 수집하고, 중복을 제거하여 데이터베이스에 쌓아주는 안정적인 파이프라인**을 완성했습니다. 이것은 우리 프로젝트의 심장과도 같은 아주 중요한 부분입니다.

이제 우리는 안정적으로 쌓이는 '데이터'라는 재료를 얻었습니다. 다음 단계는 이 재료를 가지고 맛있는 요리를 시작하는 것입니다.

- **Phase 3: AI 시스템 구축**
  - 수집된 `raw_apt_trade` 테이블의 데이터를 분석하고 정제(Cleansing)하여 AI 모델이 학습할 수 있는 피처(Feature)로 가공하는 작업을 시작합니다.
